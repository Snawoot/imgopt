#!/usr/bin/env python

import sys
import ConfigParser
import os
import os.path
import threading
import subprocess
import logging
import hashlib
import json
import sqlite3 as lite

GLOBAL = "global"
BUFFSIZE = 8 * 2**20
app_root = os.path.dirname(os.path.realpath(__file__))
app_db = os.path.join(app_root, "imgopt.db")
cfg_defaults = {
    "jpegoptim" :   "/usr/bin/jpegoptim",
    "optipng"   :   "/usr/bin/optipng",
    "log"       :   "stderr",
    "threads"   :   8,
}

ext_format = {
    "jpg"   :   "jpeg",
    "jpeg"  :   "jpeg",
    "jpe"   :   "jpeg",
    "jfif"  :   "jpeg",
    "jif"   :   "jpeg",
    "png"   :   "png",
}

threads = cfg_defaults["threads"]
thread_sem = threading.BoundedSemaphore(threads)
children = list()

def prepare_logfile(logname):
    if logname == "stdout":
        return sys.stdout
    elif logname == "stderr":
        return sys.stderr
    else:
        return open(logname, "a")

def setup_logger(logfile):
    logger = logging.getLogger("imgopt")
    logger.setLevel(logging.INFO)
    log_handler = logging.StreamHandler(logfile)
    log_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(log_handler)
    return logger

def hash_file(filename):
    with open(filename, "rb") as f:
        H = hashlib.sha1()
        while True:
            data = f.read(BUFFSIZE)
            if not data:
                break
            H.update(data)
    log.debug("SHA1(%s) = %s", repr(filename), H.hexdigest())
    return H.digest()

def process_image_threaded(img_file, callback):
    h = hash_file(img_file)
    oldsize = os.stat(img_file).st_size 
    callback(oldsize,0)
    thread_sem.release()

def process_image(*args):
    global children
    children = [ child for child in children if child.is_alive() ]
    thread_sem.acquire()
    t = threading.Thread(target = process_image_threaded, args = args)
    t.start()
    children.append(t)

def optimize_imagedir(path):
    results = {
        "oldsize": 0,
        "newsize": 0,
        "count":   0,
    }
    log.info("Processing directory \"%s\"...", path)
    def cb(before, after):
        results["oldsize"] += before
        results["newsize"] += after
        results["count"] += 1
    for dp, dn, fn in os.walk(path):
        for filename in fn:
            if os.path.splitext(filename)[1][1:].lower() in ext_format:
                #try:
                process_image(os.path.join(dp, filename), cb)
                #except KeyboardInterrupt:
                #    sys.exit(1)
                #except:
                #    pass
    for i in xrange(threads):
        thread_sem.acquire()
    for i in xrange(threads):
        thread_sem.release()
    return results["oldsize"], results["newsize"], results["count"]

def main():
    global log
    global threads
    global thread_sem
    cfgpath = os.path.join(app_root, "imgopt.ini")

    config = ConfigParser.RawConfigParser(cfg_defaults)
    configs = config.read(cfgpath)
    assert configs, "No config files read"
    sections = [ s for s in config.sections() if s != GLOBAL ]

    log = setup_logger(prepare_logfile(config.get(GLOBAL, "log")))
    log.info("Starting")
    threads = config.getint(GLOBAL, "threads")
    thread_sem = threading.BoundedSemaphore(threads)

    report = dict()
    for sect in sections:
        try:
            path = config.get(sect, "path")
        except ConfigParser.NoOptionError:
            log.warn("Config section \"%s\" has no \"path\" value specified! Skipping...", sect)
            continue

        if not os.path.isdir(path):
            log.warn("Config section \"%s\": \"path\" value %s doesn`t points to a directory! Skipping...", sect, repr(path))
            continue
        oldsize, newsize, count = optimize_imagedir(path)
        report[sect] = {
            "path": path,
            "count": count,
            "oldsize": oldsize,
            "newsize": newsize,
        }

    json.dump(report, sys.stdout)

if __name__ == '__main__':
    main()
